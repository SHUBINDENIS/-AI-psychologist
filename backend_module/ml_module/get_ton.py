import pandas as pd
import re
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –æ–¥–∏–Ω —Ä–∞–∑ (–≥–ª–æ–±–∞–ª—å–Ω–æ)
MODEL_NAME = "cointegrated/rubert-tiny-sentiment-balanced"
print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å—Å—ã–ª–æ–∫, —Ç–µ–≥–æ–≤, —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä"""
    text = str(text).lower().strip()
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text


def analyze_sentiment(text: str) -> tuple[str, float]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –∫–∞—Å—Ç–æ–º–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)"""
    if not isinstance(text, str) or not text.strip():
        return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è", 0.5
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]
        labels = ["–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è", "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è", "–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è"]
        pred_label = labels[probs.argmax()]
        custom_prob = round(probs[1] * 0.5 + probs[2], 3)
        return pred_label, custom_prob
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è", 0.5


def get_neg_neu_pos(df: pd.DataFrame, screen_name) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫ DataFrame —Å—Ç–æ–ª–±—Ü—ã sentiment –∏ custom_probability"""
    print("üöÄ –ó–∞–ø—É—â–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è get_neg_neu_pos")

    df = df.copy()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ 'text'
    if 'text' not in df.columns:
        print("‚ö†Ô∏è –í—Ö–æ–¥–Ω–æ–π DataFrame –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–æ–ª–±—Ü–∞ 'text'. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä.")
        return pd.DataFrame([{
            'user_id': screen_name,
            'date': '',
            'type': 'post',
            'attachments': '',
            'text': '',
            'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',
            'custom_probability': 0.5
        }])

    df['text'] = df['text'].astype(str).apply(clean_text)
    df = df[df['text'].str.strip() != ''].dropna(subset=['text']).drop_duplicates(subset=['text'])

    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤...")

    if df.empty:
        print("‚ö†Ô∏è –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–∏–Ω –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä.")
        return pd.DataFrame([{
            'user_id': screen_name,
            'date': '',
            'type': 'post',
            'attachments': '',
            'text': '',
            'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',
            'custom_probability': 0.5
        }])

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    df[['sentiment', 'custom_probability']] = df['text'].apply(
        lambda x: pd.Series(analyze_sentiment(x))
    )

    df = df.dropna(subset=['sentiment', 'custom_probability'])

    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Å—Ç–∞–ª–æ—Å—å —Å—Ç—Ä–æ–∫: {len(df)}")
    return df





